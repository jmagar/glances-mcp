"""Alert management tools for Glances MCP server."""

from datetime import datetime
from typing import Any, Dict, List, Optional

from fastmcp import FastMCP

from config.validation import InputValidator
from glances_mcp.services.alert_engine import AlertEngine
from glances_mcp.services.glances_client import GlancesClientPool
from glances_mcp.utils.logging import logger, performance_logger


def register_alert_management_tools(
    app: FastMCP, 
    client_pool: GlancesClientPool,
    alert_engine: AlertEngine
) -> None:
    """Register alert management tools with the MCP server."""
    
    @app.tool()
    async def check_alert_conditions(
        server_alias: Optional[str] = None,
        severity: Optional[str] = None
    ) -> Dict[str, Any]:
        """Evaluate current metrics against alert thresholds and return active alerts."""
        start_time = datetime.now()
        
        try:
            # Validate parameters
            validated_params = InputValidator.validate_tool_params(
                "check_alert_conditions",
                {
                    "server_alias": server_alias,
                    "severity": severity
                }
            )
            
            # Trigger alert evaluation
            new_alerts = await alert_engine.evaluate_rules(server_alias)
            
            # Get active alerts (filtered by parameters)
            active_alerts = alert_engine.get_active_alerts(server_alias, severity)
            
            # Get alert summary
            alert_summary = alert_engine.get_alert_summary()
            
            # Format alerts for response
            formatted_alerts = []
            for alert in active_alerts:
                formatted_alert = {
                    "id": alert.id,
                    "rule_name": alert.rule_name,
                    "server_alias": alert.server_alias,
                    "metric_path": alert.metric_path,
                    "severity": alert.severity,
                    "current_value": alert.current_value,
                    "threshold_value": alert.threshold_value,
                    "message": alert.message,
                    "timestamp": alert.timestamp.isoformat(),
                    "resolved": alert.resolved,
                    "resolved_timestamp": (
                        alert.resolved_timestamp.isoformat() 
                        if alert.resolved_timestamp else None
                    ),
                    "tags": alert.tags,
                    "age_seconds": (datetime.now() - alert.timestamp).total_seconds()
                }\n                formatted_alerts.append(formatted_alert)\n            \n            # Sort by severity and timestamp\n            severity_order = {\"critical\": 0, \"warning\": 1}\n            formatted_alerts.sort(key=lambda a: (\n                severity_order.get(a[\"severity\"], 2),\n                a[\"timestamp\"]\n            ))\n            \n            result = {\n                \"active_alerts\": formatted_alerts,\n                \"new_alerts_triggered\": len(new_alerts),\n                \"evaluation_timestamp\": datetime.now().isoformat(),\n                \"summary\": alert_summary,\n                \"filters_applied\": {\n                    \"server_alias\": server_alias,\n                    \"severity\": severity\n                }\n            }\n            \n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"check_alert_conditions\", duration_ms, True)\n            \n            return result\n        \n        except Exception as e:\n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"check_alert_conditions\", duration_ms, False)\n            logger.error(\"Error in check_alert_conditions\", server_alias=server_alias, error=str(e))\n            raise\n    \n    @app.tool()\n    async def get_alert_history(\n        server_alias: Optional[str] = None,\n        severity: Optional[str] = None,\n        hours: int = 24,\n        limit: int = 100\n    ) -> Dict[str, Any]:\n        \"\"\"Get historical alert data with filtering options.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            # Validate parameters\n            if hours < 1 or hours > 168:  # Max 7 days\n                raise ValueError(\"hours must be between 1 and 168 (7 days)\")\n            \n            if limit < 1 or limit > 1000:\n                raise ValueError(\"limit must be between 1 and 1000\")\n            \n            # Get alert history\n            historical_alerts = alert_engine.get_alert_history(\n                server_alias, severity, hours, limit\n            )\n            \n            # Format alerts for response\n            formatted_alerts = []\n            resolution_times = []\n            \n            for alert in historical_alerts:\n                formatted_alert = {\n                    \"id\": alert.id,\n                    \"rule_name\": alert.rule_name,\n                    \"server_alias\": alert.server_alias,\n                    \"metric_path\": alert.metric_path,\n                    \"severity\": alert.severity,\n                    \"current_value\": alert.current_value,\n                    \"threshold_value\": alert.threshold_value,\n                    \"message\": alert.message,\n                    \"triggered_at\": alert.timestamp.isoformat(),\n                    \"resolved\": alert.resolved,\n                    \"resolved_at\": (\n                        alert.resolved_timestamp.isoformat() \n                        if alert.resolved_timestamp else None\n                    ),\n                    \"tags\": alert.tags\n                }\n                \n                # Calculate resolution time if resolved\n                if alert.resolved and alert.resolved_timestamp:\n                    resolution_seconds = (\n                        alert.resolved_timestamp - alert.timestamp\n                    ).total_seconds()\n                    formatted_alert[\"resolution_time_seconds\"] = resolution_seconds\n                    formatted_alert[\"resolution_time_minutes\"] = resolution_seconds / 60\n                    resolution_times.append(resolution_seconds)\n                \n                formatted_alerts.append(formatted_alert)\n            \n            # Calculate statistics\n            total_alerts = len(formatted_alerts)\n            resolved_alerts = len([a for a in formatted_alerts if a[\"resolved\"]])\n            critical_alerts = len([a for a in formatted_alerts if a[\"severity\"] == \"critical\"])\n            warning_alerts = len([a for a in formatted_alerts if a[\"severity\"] == \"warning\"])\n            \n            # Calculate mean time to resolution\n            mttr_minutes = None\n            if resolution_times:\n                mttr_minutes = (sum(resolution_times) / len(resolution_times)) / 60\n            \n            # Group by server for analysis\n            alerts_by_server = {}\n            for alert in formatted_alerts:\n                server = alert[\"server_alias\"]\n                if server not in alerts_by_server:\n                    alerts_by_server[server] = []\n                alerts_by_server[server].append(alert)\n            \n            # Group by rule for analysis\n            alerts_by_rule = {}\n            for alert in formatted_alerts:\n                rule = alert[\"rule_name\"]\n                if rule not in alerts_by_rule:\n                    alerts_by_rule[rule] = []\n                alerts_by_rule[rule].append(alert)\n            \n            result = {\n                \"alerts\": formatted_alerts,\n                \"query_parameters\": {\n                    \"server_alias\": server_alias,\n                    \"severity\": severity,\n                    \"hours\": hours,\n                    \"limit\": limit\n                },\n                \"statistics\": {\n                    \"total_alerts\": total_alerts,\n                    \"resolved_alerts\": resolved_alerts,\n                    \"active_alerts\": total_alerts - resolved_alerts,\n                    \"critical_alerts\": critical_alerts,\n                    \"warning_alerts\": warning_alerts,\n                    \"resolution_rate_percent\": (\n                        (resolved_alerts / total_alerts * 100) \n                        if total_alerts > 0 else 0\n                    ),\n                    \"mean_time_to_resolution_minutes\": mttr_minutes,\n                    \"servers_with_alerts\": len(alerts_by_server),\n                    \"unique_alert_rules\": len(alerts_by_rule)\n                },\n                \"analysis\": {\n                    \"alerts_by_server\": {\n                        server: len(alerts) \n                        for server, alerts in alerts_by_server.items()\n                    },\n                    \"alerts_by_rule\": {\n                        rule: len(alerts) \n                        for rule, alerts in alerts_by_rule.items()\n                    },\n                    \"top_alerting_servers\": sorted(\n                        alerts_by_server.items(),\n                        key=lambda x: len(x[1]),\n                        reverse=True\n                    )[:5],\n                    \"most_frequent_rules\": sorted(\n                        alerts_by_rule.items(),\n                        key=lambda x: len(x[1]),\n                        reverse=True\n                    )[:5]\n                }\n            }\n            \n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"get_alert_history\", duration_ms, True)\n            \n            return result\n        \n        except Exception as e:\n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"get_alert_history\", duration_ms, False)\n            logger.error(\"Error in get_alert_history\", \n                        server_alias=server_alias, hours=hours, error=str(e))\n            raise\n    \n    @app.tool()\n    async def get_alert_summary() -> Dict[str, Any]:\n        \"\"\"Get comprehensive alert summary and statistics.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            # Get summary from alert engine\n            summary = alert_engine.get_alert_summary()\n            \n            # Get active alerts for detailed breakdown\n            active_alerts = alert_engine.get_active_alerts()\n            \n            # Enhance summary with additional details\n            recent_history = alert_engine.get_alert_history(hours=24)\n            alerts_last_hour = alert_engine.get_alert_history(hours=1)\n            \n            # Calculate trends\n            alert_trend = \"stable\"\n            if len(alerts_last_hour) > len(recent_history) / 24 * 2:  # More than 2x hourly average\n                alert_trend = \"increasing\"\n            elif len(alerts_last_hour) == 0 and len(recent_history) > 0:\n                alert_trend = \"decreasing\"\n            \n            # Categorize active alerts by age\n            now = datetime.now()\n            new_alerts = []  # < 1 hour\n            recent_alerts = []  # 1-6 hours\n            old_alerts = []  # > 6 hours\n            \n            for alert in active_alerts:\n                age_hours = (now - alert.timestamp).total_seconds() / 3600\n                if age_hours < 1:\n                    new_alerts.append(alert)\n                elif age_hours < 6:\n                    recent_alerts.append(alert)\n                else:\n                    old_alerts.append(alert)\n            \n            # Enhanced summary\n            enhanced_summary = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"alert_counts\": {\n                    \"total_active\": summary[\"total_active\"],\n                    \"critical_active\": summary[\"critical_count\"],\n                    \"warning_active\": summary[\"warning_count\"],\n                    \"new_alerts_last_hour\": len(new_alerts),\n                    \"recent_alerts_1_6h\": len(recent_alerts),\n                    \"old_alerts_over_6h\": len(old_alerts),\n                    \"alerts_last_24h\": len(recent_history)\n                },\n                \"server_impact\": {\n                    \"servers_with_alerts\": summary[\"servers_with_alerts\"],\n                    \"total_monitored_servers\": len(client_pool.get_enabled_clients()),\n                    \"percentage_servers_affected\": (\n                        (summary[\"servers_with_alerts\"] / len(client_pool.get_enabled_clients()) * 100)\n                        if len(client_pool.get_enabled_clients()) > 0 else 0\n                    ),\n                    \"top_alerting_servers\": summary[\"top_alerting_servers\"]\n                },\n                \"alert_patterns\": {\n                    \"trend_last_24h\": alert_trend,\n                    \"most_common_alerts\": summary[\"most_common_alerts\"],\n                    \"alerts_by_severity\": {\n                        \"critical\": len([a for a in recent_history if a.severity == \"critical\"]),\n                        \"warning\": len([a for a in recent_history if a.severity == \"warning\"])\n                    }\n                },\n                \"alert_health\": {\n                    \"status\": \"healthy\" if summary[\"critical_count\"] == 0 else \"critical\" if summary[\"critical_count\"] > 3 else \"warning\",\n                    \"needs_attention\": summary[\"critical_count\"] > 0 or len(old_alerts) > 0,\n                    \"stale_alerts\": len(old_alerts),\n                    \"escalation_candidates\": len([\n                        alert for alert in old_alerts \n                        if alert.severity == \"warning\" and (now - alert.timestamp).total_seconds() > 21600  # 6 hours\n                    ])\n                },\n                \"recommendations\": []\n            }\n            \n            # Generate recommendations\n            if enhanced_summary[\"alert_counts\"][\"critical_active\"] > 0:\n                enhanced_summary[\"recommendations\"].append(\n                    f\"Immediate attention required: {enhanced_summary['alert_counts']['critical_active']} critical alerts active\"\n                )\n            \n            if len(old_alerts) > 0:\n                enhanced_summary[\"recommendations\"].append(\n                    f\"Review {len(old_alerts)} long-running alerts that may need attention or threshold adjustment\"\n                )\n            \n            if alert_trend == \"increasing\":\n                enhanced_summary[\"recommendations\"].append(\n                    \"Alert frequency is increasing - investigate potential issues or adjust thresholds\"\n                )\n            \n            if enhanced_summary[\"server_impact\"][\"percentage_servers_affected\"] > 50:\n                enhanced_summary[\"recommendations\"].append(\n                    \"More than 50% of servers have alerts - investigate systemic issues\"\n                )\n            \n            if not enhanced_summary[\"recommendations\"]:\n                enhanced_summary[\"recommendations\"].append(\"No immediate action required - monitoring is healthy\")\n            \n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"get_alert_summary\", duration_ms, True)\n            \n            return enhanced_summary\n        \n        except Exception as e:\n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"get_alert_summary\", duration_ms, False)\n            logger.error(\"Error in get_alert_summary\", error=str(e))\n            raise\n    \n    @app.tool()\n    async def analyze_alert_patterns(\n        hours: int = 168,  # 7 days\n        min_occurrences: int = 3\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze patterns in alert history to identify recurring issues.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            if hours < 1 or hours > 720:  # Max 30 days\n                raise ValueError(\"hours must be between 1 and 720 (30 days)\")\n            \n            # Get extended alert history\n            alert_history = alert_engine.get_alert_history(hours=hours, limit=1000)\n            \n            if not alert_history:\n                return {\n                    \"message\": \"No alert history available for analysis\",\n                    \"analysis_period_hours\": hours,\n                    \"timestamp\": datetime.now().isoformat()\n                }\n            \n            # Pattern analysis\n            patterns = {\n                \"recurring_alerts\": {},\n                \"server_patterns\": {},\n                \"time_patterns\": {},\n                \"correlation_patterns\": []\n            }\n            \n            # Group alerts by rule and server for recurring pattern detection\n            rule_server_combinations = {}\n            for alert in alert_history:\n                key = f\"{alert.rule_name}:{alert.server_alias}\"\n                if key not in rule_server_combinations:\n                    rule_server_combinations[key] = []\n                rule_server_combinations[key].append(alert)\n            \n            # Identify recurring alerts\n            for combination, alerts in rule_server_combinations.items():\n                if len(alerts) >= min_occurrences:\n                    rule_name, server_alias = combination.split(\":\")\n                    \n                    # Calculate time between alerts\n                    sorted_alerts = sorted(alerts, key=lambda a: a.timestamp)\n                    intervals = []\n                    for i in range(1, len(sorted_alerts)):\n                        interval_hours = (\n                            sorted_alerts[i].timestamp - sorted_alerts[i-1].timestamp\n                        ).total_seconds() / 3600\n                        intervals.append(interval_hours)\n                    \n                    avg_interval = sum(intervals) / len(intervals) if intervals else 0\n                    \n                    patterns[\"recurring_alerts\"][combination] = {\n                        \"rule_name\": rule_name,\n                        \"server_alias\": server_alias,\n                        \"occurrences\": len(alerts),\n                        \"first_occurrence\": sorted_alerts[0].timestamp.isoformat(),\n                        \"last_occurrence\": sorted_alerts[-1].timestamp.isoformat(),\n                        \"average_interval_hours\": round(avg_interval, 2),\n                        \"severity_distribution\": {\n                            \"critical\": len([a for a in alerts if a.severity == \"critical\"]),\n                            \"warning\": len([a for a in alerts if a.severity == \"warning\"])\n                        },\n                        \"pattern_type\": (\n                            \"frequent\" if avg_interval < 6 else\n                            \"regular\" if avg_interval < 24 else\n                            \"periodic\"\n                        )\n                    }\n            \n            # Server-specific patterns\n            server_alert_counts = {}\n            for alert in alert_history:\n                server = alert.server_alias\n                if server not in server_alert_counts:\n                    server_alert_counts[server] = {\"total\": 0, \"critical\": 0, \"warning\": 0, \"rules\": set()}\n                \n                server_alert_counts[server][\"total\"] += 1\n                server_alert_counts[server][alert.severity] += 1\n                server_alert_counts[server][\"rules\"].add(alert.rule_name)\n            \n            # Identify problematic servers\n            for server, counts in server_alert_counts.items():\n                if counts[\"total\"] >= min_occurrences:\n                    patterns[\"server_patterns\"][server] = {\n                        \"total_alerts\": counts[\"total\"],\n                        \"critical_alerts\": counts[\"critical\"],\n                        \"warning_alerts\": counts[\"warning\"],\n                        \"unique_rules_triggered\": len(counts[\"rules\"]),\n                        \"alert_density\": counts[\"total\"] / hours,  # alerts per hour\n                        \"severity_ratio\": (\n                            counts[\"critical\"] / counts[\"total\"]\n                            if counts[\"total\"] > 0 else 0\n                        )\n                    }\n            \n            # Time-based patterns (hour of day analysis)\n            hourly_distribution = [0] * 24\n            for alert in alert_history:\n                hour = alert.timestamp.hour\n                hourly_distribution[hour] += 1\n            \n            # Find peak hours\n            peak_hours = []\n            avg_hourly = sum(hourly_distribution) / 24\n            for hour, count in enumerate(hourly_distribution):\n                if count > avg_hourly * 1.5:  # 50% above average\n                    peak_hours.append({\"hour\": hour, \"alert_count\": count})\n            \n            patterns[\"time_patterns\"] = {\n                \"hourly_distribution\": hourly_distribution,\n                \"peak_hours\": sorted(peak_hours, key=lambda x: x[\"alert_count\"], reverse=True),\n                \"busiest_hour\": hourly_distribution.index(max(hourly_distribution)),\n                \"quietest_hour\": hourly_distribution.index(min(hourly_distribution))\n            }\n            \n            # Generate insights and recommendations\n            insights = []\n            recommendations = []\n            \n            if patterns[\"recurring_alerts\"]:\n                frequent_alerts = [\n                    p for p in patterns[\"recurring_alerts\"].values()\n                    if p[\"pattern_type\"] == \"frequent\"\n                ]\n                if frequent_alerts:\n                    insights.append(\n                        f\"Found {len(frequent_alerts)} frequently recurring alert patterns (< 6h intervals)\"\n                    )\n                    recommendations.append(\"Review alert thresholds for frequently recurring alerts\")\n            \n            if patterns[\"server_patterns\"]:\n                high_density_servers = [\n                    (server, data) for server, data in patterns[\"server_patterns\"].items()\n                    if data[\"alert_density\"] > 1  # More than 1 alert per hour on average\n                ]\n                if high_density_servers:\n                    insights.append(\n                        f\"Identified {len(high_density_servers)} servers with high alert density\"\n                    )\n                    recommendations.append(\"Investigate servers with consistently high alert volumes\")\n            \n            if patterns[\"time_patterns\"][\"peak_hours\"]:\n                insights.append(\n                    f\"Alert activity peaks at hour {patterns['time_patterns']['busiest_hour']}:00\"\n                )\n                recommendations.append(\"Consider scheduled maintenance during quieter hours\")\n            \n            result = {\n                \"analysis_summary\": {\n                    \"total_alerts_analyzed\": len(alert_history),\n                    \"analysis_period_hours\": hours,\n                    \"recurring_patterns_found\": len(patterns[\"recurring_alerts\"]),\n                    \"servers_with_patterns\": len(patterns[\"server_patterns\"]),\n                    \"time_patterns_detected\": len(patterns[\"time_patterns\"][\"peak_hours\"]),\n                    \"timestamp\": datetime.now().isoformat()\n                },\n                \"patterns\": patterns,\n                \"insights\": insights,\n                \"recommendations\": recommendations\n            }\n            \n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"analyze_alert_patterns\", duration_ms, True)\n            \n            return result\n        \n        except Exception as e:\n            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n            performance_logger.log_tool_execution(\"analyze_alert_patterns\", duration_ms, False)\n            logger.error(\"Error in analyze_alert_patterns\", hours=hours, error=str(e))\n            raise